epoch: 2  

#LearningRate:
#  schedulers:
#    - !PiecewiseDecay
#      gamma: 0.1
#      milestones: [32, 36]
#      use_warmup: False
#  - !LinearWarmup
#    start_factor: 0.3333333333333333
#    steps: 100

OptimizerBuilder:
  optimizer:
    type: "SGD"
    #lr: 0.01
    # momentum: 0.9
    # weight_decay: 0.0005
